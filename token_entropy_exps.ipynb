{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdba8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_safeset(path: str) -> set:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pkl.load(f)\n",
    "\n",
    "\n",
    "def filterize(texts: Iterable[str], safeset: set) -> List[str]:\n",
    "    return [t for t in texts if t and len(set(t).difference(safeset)) == 0]\n",
    "\n",
    "\n",
    "def load_texts(files: Sequence[str]) -> List[str]:\n",
    "    data = []\n",
    "    for path in files:\n",
    "        with open(path, \"r\") as f:\n",
    "            data.extend(json.loads(line)[\"text\"] for line in f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def chunk_texts(texts: Sequence[str], splitlen: int) -> List[str]:\n",
    "    windows: List[str] = []\n",
    "    for t in texts:\n",
    "        if not t:\n",
    "            continue\n",
    "        windows.extend(\n",
    "            t[i : i + splitlen] for i in np.arange(0, len(t), splitlen) if t[i : i + splitlen]\n",
    "        )\n",
    "    return windows\n",
    "\n",
    "\n",
    "def prepare_samples(\n",
    "    files: Sequence[str],\n",
    "    safeset: set,\n",
    "    splitlen: int,\n",
    "    nsamples: int,\n",
    "    buffer: int,\n",
    "    seed: int,\n",
    ") -> List[str]:\n",
    "    btext = load_texts(files)\n",
    "    ftext = filterize(btext, safeset)\n",
    "    ftext_chunked = chunk_texts(ftext, splitlen)\n",
    "\n",
    "    lengths = np.array([len(t) for t in ftext_chunked])\n",
    "    indsort = np.flip(np.argsort(lengths))\n",
    "\n",
    "    sample_count = min(nsamples, len(indsort))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    starts = rng.choice(buffer, size=sample_count) if buffer > 0 else np.zeros(sample_count, dtype=int)\n",
    "    ftext_sorted = [ftext_chunked[i][s:] for i, s in zip(indsort[:sample_count], starts)]\n",
    "    return ftext_sorted\n",
    "\n",
    "\n",
    "def build_safe_token_ids(tokenizer, safeset: set) -> np.ndarray:\n",
    "    safe_ids = []\n",
    "    for tid in range(len(tokenizer)):\n",
    "        text = tokenizer.decode(tid, clean_up_tokenization_spaces=False)\n",
    "        if text and set(text).issubset(safeset):\n",
    "            safe_ids.append(tid)\n",
    "    return np.array(safe_ids, dtype=np.int64)\n",
    "\n",
    "\n",
    "def rows_all_safe(tok_batch: np.ndarray, attn_batch: np.ndarray, safe_id_set: set) -> np.ndarray:\n",
    "    active = attn_batch == 1  # ignore padding\n",
    "    return np.array([all((tid in safe_id_set) for tid in seq[mask]) for seq, mask in zip(tok_batch, active)])\n",
    "\n",
    "\n",
    "def compute_entropies(\n",
    "    dataloader: DataLoader,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    shapecut: int,\n",
    "    context_lengths: Sequence[int],\n",
    "    near_zero_thresh: float,\n",
    "    safe_id_set: set,\n",
    "    top_k: int,\n",
    ") -> Tuple[Dict[int, np.ndarray], List[Tuple[int, int, str]]]:\n",
    "    ent_by_ctx: Dict[int, List[np.ndarray]] = {k: [] for k in context_lengths}\n",
    "    zero_bin_tokens: List[int] = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for docs in tqdm(dataloader):\n",
    "            inputs = tokenizer(\n",
    "                docs,\n",
    "                return_tensors=\"pt\",\n",
    "                return_token_type_ids=False,\n",
    "                padding=\"max_length\",\n",
    "                max_length=shapecut,\n",
    "                truncation=True,\n",
    "            ).to(device)\n",
    "\n",
    "            logits = model(**inputs).logits  # [B, T, V]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            tok = inputs[\"input_ids\"].cpu().numpy()\n",
    "            attn = inputs[\"attention_mask\"].cpu().numpy()\n",
    "\n",
    "            keep_rows = rows_all_safe(tok, attn, safe_id_set)\n",
    "            if not keep_rows.any():\n",
    "                continue\n",
    "\n",
    "            tok = tok[keep_rows]\n",
    "            attn = attn[keep_rows]\n",
    "            probs = probs[keep_rows]\n",
    "\n",
    "            token_entropy = -(probs * torch.log2(probs.clamp_min(1e-10))).sum(dim=-1)\n",
    "\n",
    "            for k in context_lengths:\n",
    "                if k > token_entropy.shape[1]:\n",
    "                    continue\n",
    "                ent_slice = token_entropy[:, k - 1].cpu().numpy()\n",
    "                attn_slice = attn[:, k - 1]\n",
    "                ent_valid = ent_slice[attn_slice == 1]\n",
    "                if not ent_valid.size:\n",
    "                    continue\n",
    "                ent_by_ctx[k].append(ent_valid)\n",
    "                near_zero_mask = ent_valid < near_zero_thresh\n",
    "                if near_zero_mask.any():\n",
    "                    zero_bin_tokens.extend(tok[:, k - 1][attn_slice == 1][near_zero_mask])\n",
    "\n",
    "    ent_by_ctx = {k: (np.concatenate(v) if v else np.array([])) for k, v in ent_by_ctx.items()}\n",
    "\n",
    "    if zero_bin_tokens:\n",
    "        uniq, counts = np.unique(zero_bin_tokens, return_counts=True)\n",
    "        zero_entropy_tokens = sorted(zip(uniq, counts), key=lambda x: x[1], reverse=True)\n",
    "        top_zero_tokens = [\n",
    "            (tid, cnt, tokenizer.decode(tid, clean_up_tokenization_spaces=False))\n",
    "            for tid, cnt in zero_entropy_tokens[:top_k]\n",
    "        ]\n",
    "    else:\n",
    "        top_zero_tokens = []\n",
    "\n",
    "    return ent_by_ctx, top_zero_tokens\n",
    "\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"Compute next-token entropy distributions with safelist filtering.\")\n",
    "    parser.add_argument(\n",
    "        \"--files\",\n",
    "        nargs=\"+\",\n",
    "        default=[\n",
    "            \"/scratch/gpfs/DATASETS/hugging_face/c4/en/c4-train.00217-of-01024.json\",\n",
    "        ],\n",
    "        help=\"JSONL files to read (expects a 'text' field per line).\",\n",
    "    )\n",
    "    parser.add_argument(\"--safeset-path\", type=str, default=\"colin_files/safeset2.txt\", help=\"Pickled safeset path.\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"allenai/OLMo-2-0425-1B\", help=\"HF model name.\")\n",
    "    parser.add_argument(\"--splitlen\", type=int, default=15000, help=\"Non-overlapping window length for chunking text.\")\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=2000, help=\"How many windows to keep (after sorting by length).\")\n",
    "    parser.add_argument(\"--buffer\", type=int, default=100, help=\"Random start offset range for each selected window.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=223291, help=\"RNG seed for start offsets.\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=2, help=\"Dataloader batch size.\")\n",
    "    parser.add_argument(\"--shapecut\", type=int, default=1024, help=\"Max tokens per sample (padding/truncation).\")\n",
    "    parser.add_argument(\n",
    "        \"--context-lengths\",\n",
    "        type=str,\n",
    "        default=\"1,3,10,40,100,1000\",\n",
    "        help=\"Comma-separated context lengths to sample entropy from.\",\n",
    "    )\n",
    "    parser.add_argument(\"--near-zero-thresh\", type=float, default=1e-3, help=\"Threshold for ~zero-entropy bin.\")\n",
    "    parser.add_argument(\"--top-k\", type=int, default=30, help=\"How many zero-entropy tokens to display.\")\n",
    "    parser.add_argument(\n",
    "        \"--save-ent\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Optional path to save entropies as npz (keys ctx_<len>).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save-zero\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Optional path to save zero-entropy tokens as TSV (id, count, text).\",\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "context_lengths = [int(x) for x in args.context_lengths.split(\",\") if x]\n",
    "if args.shapecut < max(context_lengths):\n",
    "    raise ValueError(f\"shapecut {args.shapecut} must be >= max context length {max(context_lengths)}\")\n",
    "\n",
    "safeset = load_safeset(args.safeset_path)\n",
    "\n",
    "print(\"Preparing samples...\")\n",
    "ftext_sorted = prepare_samples(\n",
    "    files=args.files,\n",
    "    safeset=safeset,\n",
    "    splitlen=args.splitlen,\n",
    "    nsamples=args.nsamples,\n",
    "    buffer=args.buffer,\n",
    "    seed=args.seed,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model, device_map=\"auto\")\n",
    "model.to(device)\n",
    "\n",
    "dataloader = DataLoader(ftext_sorted, batch_size=args.batch_size)\n",
    "\n",
    "safe_ids = build_safe_token_ids(tokenizer, safeset)\n",
    "safe_id_set = set(safe_ids.tolist())\n",
    "\n",
    "print(\"Computing entropies...\")\n",
    "ent_by_ctx, top_zero_tokens = compute_entropies(\n",
    "    dataloader=dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    shapecut=args.shapecut,\n",
    "    context_lengths=context_lengths,\n",
    "    near_zero_thresh=args.near_zero_thresh,\n",
    "    safe_id_set=safe_id_set,\n",
    "    top_k=args.top_k,\n",
    ")\n",
    "\n",
    "sizes = {k: v.size for k, v in ent_by_ctx.items()}\n",
    "print(\"Entropy counts per context length:\", sizes)\n",
    "print(\"Top zero-entropy tokens (id, count, text):\", top_zero_tokens)\n",
    "\n",
    "if args.save_ent:\n",
    "    save_dict = {f\"ctx_{k}\": v for k, v in ent_by_ctx.items()}\n",
    "    np.savez_compressed(args.save_ent, **save_dict)\n",
    "    print(f\"Saved entropies to {args.save_ent}\")\n",
    "\n",
    "if args.save_zero and top_zero_tokens:\n",
    "    with open(args.save_zero, \"w\") as f:\n",
    "        for tid, cnt, text in top_zero_tokens:\n",
    "            safe_text = text.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "            f.write(f\"{tid}\\t{cnt}\\t{safe_text}\\n\")\n",
    "    print(f\"Saved zero-entropy tokens to {args.save_zero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee35093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 356317/356317 [00:07<00:00, 47594.82it/s]\n"
     ]
    }
   ],
   "source": [
    "files=['/scratch/gpfs/DATASETS/hugging_face/c4/en/c4-train.00217-of-01024.json',\n",
    "    #    '/scratch/gpfs/DATASETS/hugging_face/c4/en/c4-train.00023-of-01024.json',\n",
    "    #    '/scratch/gpfs/DATASETS/hugging_face/c4/en/c4-train.00345-of-01024.json'\n",
    "]\n",
    "\n",
    "data=[]\n",
    "for file in files:\n",
    "    with open(file,'r') as f:\n",
    "        data+=[json.loads(l) for l in f]\n",
    "\n",
    "btext=[d['text'] for d in data]\n",
    "\n",
    "ftext = filterize(btext)\n",
    "\n",
    "# chunk each filtered doc into non-overlapping windows so we maximize usable samples\n",
    "splitlen = 15000\n",
    "def chunk_texts(texts, splitlen):\n",
    "    windows = []\n",
    "    for t in texts:\n",
    "        if not t:\n",
    "            continue\n",
    "        windows.extend([t[i:i+splitlen] for i in np.arange(0, len(t), splitlen) if t[i:i+splitlen]])\n",
    "    return windows\n",
    "\n",
    "ftext_chunked = chunk_texts(ftext, splitlen)\n",
    "\n",
    "lengths=np.array([len(t) for t in ftext_chunked])\n",
    "indsort=np.flip(np.argsort(lengths))\n",
    "nsamples=2000\n",
    "buffer=100\n",
    "rng=np.random.default_rng(223291)\n",
    "starts=rng.choice(buffer,size=nsamples)\n",
    "ftext_sorted=[ftext_chunked[i][s:] for i,s in zip(indsort[:nsamples],starts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5114d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Olmo2ForCausalLM(\n",
       "  (model): Olmo2Model(\n",
       "    (embed_tokens): Embedding(100352, 2048, padding_idx=100277)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x Olmo2DecoderLayer(\n",
       "        (self_attn): Olmo2Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "          (k_norm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Olmo2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (post_attention_layernorm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Olmo2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Olmo2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'allenai/OLMo-2-0425-1B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, device_map='auto')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model, device_map='auto')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dataloader = DataLoader(ftext_sorted, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d695616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of token entropies depending on how much context is given to the model\n",
    "\n",
    "# similar to this code\n",
    "# for step,ax in zip(steps,axes[:,i]):       \n",
    "#     msk=data['m'][:,step+1].astype(bool)\n",
    "#     ax.hist(data['e'][msk,step+1],density=True,color='k',bins=bins);\n",
    "#     ax.axvline(avs['e'][step+1],c='gray',zorder=-1,ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_safeset(path: str) -> set:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pkl.load(f)\n",
    "\n",
    "\n",
    "def filterize(texts: Iterable[str], safeset: set) -> List[str]:\n",
    "    return [t for t in texts if t and len(set(t).difference(safeset)) == 0]\n",
    "\n",
    "\n",
    "def load_texts(files: Sequence[str]) -> List[str]:\n",
    "    data = []\n",
    "    for path in files:\n",
    "        with open(path, \"r\") as f:\n",
    "            data.extend(json.loads(line)[\"text\"] for line in f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def chunk_texts(texts: Sequence[str], splitlen: int) -> List[str]:\n",
    "    windows: List[str] = []\n",
    "    for t in texts:\n",
    "        if not t:\n",
    "            continue\n",
    "        windows.extend(\n",
    "            t[i : i + splitlen] for i in np.arange(0, len(t), splitlen) if t[i : i + splitlen]\n",
    "        )\n",
    "    return windows\n",
    "\n",
    "\n",
    "def prepare_samples(\n",
    "    files: Sequence[str],\n",
    "    safeset: set,\n",
    "    splitlen: int,\n",
    "    nsamples: int,\n",
    "    buffer: int,\n",
    "    seed: int,\n",
    ") -> List[str]:\n",
    "    btext = load_texts(files)\n",
    "    ftext = filterize(btext, safeset)\n",
    "    ftext_chunked = chunk_texts(ftext, splitlen)\n",
    "\n",
    "    lengths = np.array([len(t) for t in ftext_chunked])\n",
    "    indsort = np.flip(np.argsort(lengths))\n",
    "\n",
    "    sample_count = min(nsamples, len(indsort))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    starts = rng.choice(buffer, size=sample_count) if buffer > 0 else np.zeros(sample_count, dtype=int)\n",
    "    ftext_sorted = [ftext_chunked[i][s:] for i, s in zip(indsort[:sample_count], starts)]\n",
    "    return ftext_sorted\n",
    "\n",
    "\n",
    "def build_safe_token_ids(tokenizer, safeset: set) -> np.ndarray:\n",
    "    safe_ids = []\n",
    "    for tid in range(len(tokenizer)):\n",
    "        text = tokenizer.decode(tid, clean_up_tokenization_spaces=False)\n",
    "        if text and set(text).issubset(safeset):\n",
    "            safe_ids.append(tid)\n",
    "    return np.array(safe_ids, dtype=np.int64)\n",
    "\n",
    "\n",
    "def rows_all_safe(tok_batch: np.ndarray, attn_batch: np.ndarray, safe_id_set: set) -> np.ndarray:\n",
    "    active = attn_batch == 1  # ignore padding\n",
    "    return np.array([all((tid in safe_id_set) for tid in seq[mask]) for seq, mask in zip(tok_batch, active)])\n",
    "\n",
    "\n",
    "def compute_entropies(\n",
    "    dataloader: DataLoader,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    shapecut: int,\n",
    "    context_lengths: Sequence[int],\n",
    "    near_zero_thresh: float,\n",
    "    safe_id_set: set,\n",
    "    top_k: int,\n",
    ") -> Tuple[Dict[int, np.ndarray], List[Tuple[int, int, str]]]:\n",
    "    ent_by_ctx: Dict[int, List[np.ndarray]] = {k: [] for k in context_lengths}\n",
    "    zero_bin_tokens: List[int] = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for docs in tqdm(dataloader):\n",
    "            inputs = tokenizer(\n",
    "                docs,\n",
    "                return_tensors=\"pt\",\n",
    "                return_token_type_ids=False,\n",
    "                padding=\"max_length\",\n",
    "                max_length=shapecut,\n",
    "                truncation=True,\n",
    "            ).to(device)\n",
    "\n",
    "            logits = model(**inputs).logits  # [B, T, V]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            tok = inputs[\"input_ids\"].cpu().numpy()\n",
    "            attn = inputs[\"attention_mask\"].cpu().numpy()\n",
    "\n",
    "            keep_rows = rows_all_safe(tok, attn, safe_id_set)\n",
    "            if not keep_rows.any():\n",
    "                continue\n",
    "\n",
    "            tok = tok[keep_rows]\n",
    "            attn = attn[keep_rows]\n",
    "            probs = probs[keep_rows]\n",
    "\n",
    "            token_entropy = -(probs * torch.log2(probs.clamp_min(1e-10))).sum(dim=-1)\n",
    "\n",
    "            for k in context_lengths:\n",
    "                if k > token_entropy.shape[1]:\n",
    "                    continue\n",
    "                ent_slice = token_entropy[:, k - 1].cpu().numpy()\n",
    "                attn_slice = attn[:, k - 1]\n",
    "                ent_valid = ent_slice[attn_slice == 1]\n",
    "                if not ent_valid.size:\n",
    "                    continue\n",
    "                ent_by_ctx[k].append(ent_valid)\n",
    "                near_zero_mask = ent_valid < near_zero_thresh\n",
    "                if near_zero_mask.any():\n",
    "                    zero_bin_tokens.extend(tok[:, k - 1][attn_slice == 1][near_zero_mask])\n",
    "\n",
    "    ent_by_ctx = {k: (np.concatenate(v) if v else np.array([])) for k, v in ent_by_ctx.items()}\n",
    "\n",
    "    if zero_bin_tokens:\n",
    "        uniq, counts = np.unique(zero_bin_tokens, return_counts=True)\n",
    "        zero_entropy_tokens = sorted(zip(uniq, counts), key=lambda x: x[1], reverse=True)\n",
    "        top_zero_tokens = [\n",
    "            (tid, cnt, tokenizer.decode(tid, clean_up_tokenization_spaces=False))\n",
    "            for tid, cnt in zero_entropy_tokens[:top_k]\n",
    "        ]\n",
    "    else:\n",
    "        top_zero_tokens = []\n",
    "\n",
    "    return ent_by_ctx, top_zero_tokens\n",
    "\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"Compute next-token entropy distributions with safelist filtering.\")\n",
    "    parser.add_argument(\n",
    "        \"--files\",\n",
    "        nargs=\"+\",\n",
    "        default=[\n",
    "            \"/scratch/gpfs/DATASETS/hugging_face/c4/en/c4-train.00217-of-01024.json\",\n",
    "        ],\n",
    "        help=\"JSONL files to read (expects a 'text' field per line).\",\n",
    "    )\n",
    "    parser.add_argument(\"--safeset-path\", type=str, default=\"colin_files/safeset2.txt\", help=\"Pickled safeset path.\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"allenai/OLMo-2-0425-1B\", help=\"HF model name.\")\n",
    "    parser.add_argument(\"--splitlen\", type=int, default=15000, help=\"Non-overlapping window length for chunking text.\")\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=2000, help=\"How many windows to keep (after sorting by length).\")\n",
    "    parser.add_argument(\"--buffer\", type=int, default=100, help=\"Random start offset range for each selected window.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=223291, help=\"RNG seed for start offsets.\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=2, help=\"Dataloader batch size.\")\n",
    "    parser.add_argument(\"--shapecut\", type=int, default=1024, help=\"Max tokens per sample (padding/truncation).\")\n",
    "    parser.add_argument(\n",
    "        \"--context-lengths\",\n",
    "        type=str,\n",
    "        default=\"1,3,10,40,100,1000\",\n",
    "        help=\"Comma-separated context lengths to sample entropy from.\",\n",
    "    )\n",
    "    parser.add_argument(\"--near-zero-thresh\", type=float, default=1e-3, help=\"Threshold for ~zero-entropy bin.\")\n",
    "    parser.add_argument(\"--top-k\", type=int, default=30, help=\"How many zero-entropy tokens to display.\")\n",
    "    parser.add_argument(\n",
    "        \"--save-ent\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Optional path to save entropies as npz (keys ctx_<len>).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save-zero\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Optional path to save zero-entropy tokens as TSV (id, count, text).\",\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    context_lengths = [int(x) for x in args.context_lengths.split(\",\") if x]\n",
    "    if args.shapecut < max(context_lengths):\n",
    "        raise ValueError(f\"shapecut {args.shapecut} must be >= max context length {max(context_lengths)}\")\n",
    "\n",
    "    safeset = load_safeset(args.safeset_path)\n",
    "\n",
    "    print(\"Preparing samples...\")\n",
    "    ftext_sorted = prepare_samples(\n",
    "        files=args.files,\n",
    "        safeset=safeset,\n",
    "        splitlen=args.splitlen,\n",
    "        nsamples=args.nsamples,\n",
    "        buffer=args.buffer,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model, device_map=\"auto\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model, device_map=\"auto\")\n",
    "    model.to(device)\n",
    "\n",
    "    dataloader = DataLoader(ftext_sorted, batch_size=args.batch_size)\n",
    "\n",
    "    safe_ids = build_safe_token_ids(tokenizer, safeset)\n",
    "    safe_id_set = set(safe_ids.tolist())\n",
    "\n",
    "    print(\"Computing entropies...\")\n",
    "    ent_by_ctx, top_zero_tokens = compute_entropies(\n",
    "        dataloader=dataloader,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        shapecut=args.shapecut,\n",
    "        context_lengths=context_lengths,\n",
    "        near_zero_thresh=args.near_zero_thresh,\n",
    "        safe_id_set=safe_id_set,\n",
    "        top_k=args.top_k,\n",
    "    )\n",
    "\n",
    "    sizes = {k: v.size for k, v in ent_by_ctx.items()}\n",
    "    print(\"Entropy counts per context length:\", sizes)\n",
    "    print(\"Top zero-entropy tokens (id, count, text):\", top_zero_tokens)\n",
    "\n",
    "    if args.save_ent:\n",
    "        save_dict = {f\"ctx_{k}\": v for k, v in ent_by_ctx.items()}\n",
    "        np.savez_compressed(args.save_ent, **save_dict)\n",
    "        print(f\"Saved entropies to {args.save_ent}\")\n",
    "\n",
    "    if args.save_zero and top_zero_tokens:\n",
    "        with open(args.save_zero, \"w\") as f:\n",
    "            for tid, cnt, text in top_zero_tokens:\n",
    "                safe_text = text.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "                f.write(f\"{tid}\\t{cnt}\\t{safe_text}\\n\")\n",
    "        print(f\"Saved zero-entropy tokens to {args.save_zero}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
